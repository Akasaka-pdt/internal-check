
import streamlit as st
import pandas as pd
import plotly.express as px
import io
from datetime import datetime

month_order = ['4æœˆå·', '5æœˆå·', '6æœˆå·', '7æœˆå·', '8æœˆå·', '9æœˆå·', '10æœˆå·', '11æœˆå·', '12æœˆå·', '1æœˆå·', '2æœˆå·', '3æœˆå·', 'ãã®ä»–']

# å·¥ç¨‹ã®é †åºï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰ã‚’å®šç¾©
original_process_order = ['ä»®å°å‰²', 'å…¥ç¨¿å‰ãƒ©ãƒ•', 'å…¥ç¨¿åŸç¨¿', 'çµ„ç‰ˆåŸç¨¿', 'åˆæ ¡', 'å†æ ¡', 'å†æ ¡2', 'å†æ ¡3',
                        'è‰²æ ¡', 'è‰²æ ¡2', 'è‰²æ ¡3', 'å¿µæ ¡', 'å¿µæ ¡2', 'å¿µæ ¡3', 'Î±1ç‰ˆ', 'Î²1ç‰ˆ', 'Î²2ç‰ˆ', 'Î²3ç‰ˆ', 'Î²4ç‰ˆ', 'Î²5ç‰ˆ', 'ãã®ä»–']
# é †åºã‚’ä¿ã¡ã¤ã¤é‡è¤‡å‰Šé™¤
original_process_order = list(dict.fromkeys(original_process_order))


# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(
    page_title="ç¤¾å†…ãƒã‚§ãƒƒã‚¯æ¥­å‹™ BPRåˆ†æãƒ„ãƒ¼ãƒ«",
    layout="wide"
)

st.title("ğŸ“Š ç¤¾å†…ãƒã‚§ãƒƒã‚¯æ¥­å‹™ BPRåˆ†æãƒ„ãƒ¼ãƒ«")

# --- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ --- 
st.sidebar.header("1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
st.sidebar.info("åˆ†æå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’2ã¤ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

uploaded_seisakubutsu_file = st.sidebar.file_uploader("åˆ¶ä½œç‰©ä¸€è¦§ CSV", type="csv")
uploaded_header_file = st.sidebar.file_uploader("ãƒ˜ãƒƒãƒ€ãƒ¼ä¸€è¦§ CSV", type="csv")

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ ---
@st.cache_data
def load_data(seisakubutsu_file, header_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ¼ã‚¸ã—ã¦å‰å‡¦ç†ã‚’è¡Œã†"""
    try:
        seisakubutsu_df = pd.read_csv(seisakubutsu_file)
        header_df = pd.read_csv(header_file)
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°: {e}")
        return None, None

    # --- ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† ---
    date_cols = ['ä½œæˆæ—¥', 'ä¿®æ­£æ—¥', 'ç· ã‚åˆ‡ã‚Šæ—¥']
    for col in date_cols:
        if col in seisakubutsu_df.columns:
            seisakubutsu_df[col] = pd.to_datetime(seisakubutsu_df[col], errors='coerce').dt.tz_localize(None)
        if col in header_df.columns:
            header_df[col] = pd.to_datetime(header_df[col], errors='coerce').dt.tz_localize(None)

    header_df.rename(columns={'åˆ¶ä½œç‰©ãƒˆãƒ¼ã‚¯ãƒ³': 'ãƒˆãƒ¼ã‚¯ãƒ³'}, inplace=True)
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«ãƒã‚§ãƒƒã‚¯è€…æ•°ã‚’é›†è¨ˆ
    checkers_count_df = header_df.groupby('ãƒˆãƒ¼ã‚¯ãƒ³')['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].nunique().reset_index()
    checkers_count_df.rename(columns={'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒã‚§ãƒƒã‚¯è€…æ•°'}, inplace=True)

    # åˆ¶ä½œç‰©ãƒ‡ãƒ¼ã‚¿ã¨ãƒã‚§ãƒƒã‚¯è€…æ•°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
    seisakubutsu_df = pd.merge(seisakubutsu_df, checkers_count_df, on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left')
    seisakubutsu_df['ãƒã‚§ãƒƒã‚¯è€…æ•°'].fillna(0, inplace=True) # ãƒã‚§ãƒƒã‚¯è€…ãŒã„ãªã„å ´åˆã¯0ã‚’ã‚»ãƒƒãƒˆ

    merged_df = pd.merge(header_df, seisakubutsu_df, on='ãƒˆãƒ¼ã‚¯ãƒ³', suffixes=('_header', '_seisakubutsu'))
    
    # ç™ºåˆŠæœˆã®é †åºã‚’å®šç¾©
    # ç™ºåˆŠæœˆã‚’ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›ã—ã€é †åºã‚’é©ç”¨
    merged_df['ç™ºåˆŠæœˆ'] = pd.Categorical(merged_df['ç™ºåˆŠæœˆ'], categories=month_order, ordered=True)
    seisakubutsu_df['ç™ºåˆŠæœˆ'] = pd.Categorical(seisakubutsu_df['ç™ºåˆŠæœˆ'], categories=month_order, ordered=True)

    # dfã«å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹å·¥ç¨‹ã ã‘ã‚’æ®‹ã™
    existing_processes = pd.concat([merged_df['å·¥ç¨‹'], seisakubutsu_df['å·¥ç¨‹']]).dropna().unique()
    filtered_process_order = [p for p in original_process_order if p in existing_processes]

    # Categorical ã«é©ç”¨
    merged_df['å·¥ç¨‹'] = pd.Categorical(merged_df['å·¥ç¨‹'], categories=filtered_process_order, ordered=True)
    seisakubutsu_df['å·¥ç¨‹'] = pd.Categorical(seisakubutsu_df['å·¥ç¨‹'], categories=filtered_process_order, ordered=True)

    return merged_df, seisakubutsu_df

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† --- 
if uploaded_seisakubutsu_file is not None and uploaded_header_file is not None:
    df_merged_all, df_seisakubutsu_all = load_data(uploaded_seisakubutsu_file, uploaded_header_file)

    if df_merged_all is None:
        st.stop()

    st.sidebar.header("2. çµã‚Šè¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    # --- æœŸé–“æŒ‡å®šãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ---
    st.sidebar.subheader("æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    min_date = df_seisakubutsu_all['ä½œæˆæ—¥'].min().date()
    max_date = df_seisakubutsu_all['ä½œæˆæ—¥'].max().date()
    start_date = st.sidebar.date_input('é–‹å§‹æ—¥', min_date, min_value=min_date, max_value=max_date)
    end_date = st.sidebar.date_input('çµ‚äº†æ—¥', max_date, min_value=start_date, max_value=max_date)

    if start_date > end_date:
        st.sidebar.error('ã‚¨ãƒ©ãƒ¼: çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚')
        st.stop()

    start_datetime = pd.to_datetime(start_date)
    end_datetime = pd.to_datetime(end_date) + pd.Timedelta(days=1)

    # --- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ --- 
    df_filtered_by_date = df_merged_all[(df_merged_all['ä½œæˆæ—¥_seisakubutsu'] >= start_datetime) & (df_merged_all['ä½œæˆæ—¥_seisakubutsu'] < end_datetime)].copy()
    df_seisakubutsu_filtered_by_date = df_seisakubutsu_all[(df_seisakubutsu_all['ä½œæˆæ—¥'] >= start_datetime) & (df_seisakubutsu_all['ä½œæˆæ—¥'] < end_datetime)].copy()

    st.sidebar.subheader("ç™ºåˆŠå¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    available_years = sorted(df_seisakubutsu_filtered_by_date['å¹´åº¦'].dropna().unique().tolist())
    selected_year = st.sidebar.selectbox('æ¯”è¼ƒã—ãŸã„ç™ºåˆŠå¹´åº¦ã‚’é¸æŠ', options=['ã™ã¹ã¦'] + available_years)

    # --- ç™ºåˆŠæœˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ---
    st.sidebar.subheader("ç™ºåˆŠæœˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    available_months = [m for m in month_order if m in df_seisakubutsu_filtered_by_date['ç™ºåˆŠæœˆ'].dropna().unique()]
    selected_month = st.sidebar.selectbox('æ¯”è¼ƒã—ãŸã„ç™ºåˆŠæœˆã‚’é¸æŠ', options=['ã™ã¹ã¦'] + available_months)


    df_filtered_by_month = df_filtered_by_date.copy()
    df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_date.copy()

    if selected_year != 'ã™ã¹ã¦':
        df_filtered_by_month = df_filtered_by_month[df_filtered_by_month['å¹´åº¦'] == selected_year]
        df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['å¹´åº¦'] == selected_year]

    if selected_month != 'ã™ã¹ã¦':
        df_filtered_by_month = df_filtered_by_month[df_filtered_by_month['ç™ºåˆŠæœˆ'] == selected_month]
        df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['ç™ºåˆŠæœˆ'] == selected_month]

    # --- å­¦å¹´ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ---
    st.sidebar.subheader("å­¦å¹´ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    grade_cols = [col for col in df_seisakubutsu_all.columns if 'å¹´ç”Ÿ' in col or 'å­¦å¹´ãã®ä»–' in col]
    melted_grades = df_seisakubutsu_filtered_by_month.melt(id_vars=['ãƒˆãƒ¼ã‚¯ãƒ³'], value_vars=grade_cols, var_name='å­¦å¹´', value_name='å¯¾è±¡')
    relevant_grades = melted_grades[melted_grades['å¯¾è±¡'] == True]
    available_grades = relevant_grades['å­¦å¹´'].unique().tolist()
    selected_grades = st.sidebar.multiselect('åˆ†æã—ãŸã„å­¦å¹´ã‚’é¸æŠ', options=available_grades, default=available_grades)

    if selected_grades:
        selected_tokens = relevant_grades[relevant_grades['å­¦å¹´'].isin(selected_grades)]['ãƒˆãƒ¼ã‚¯ãƒ³'].unique()
        df_filtered = df_filtered_by_month[df_filtered_by_month['ãƒˆãƒ¼ã‚¯ãƒ³'].isin(selected_tokens)].copy()
        df_seisakubutsu_filtered = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['ãƒˆãƒ¼ã‚¯ãƒ³'].isin(selected_tokens)].copy()
    else:
        df_filtered = pd.DataFrame(columns=df_filtered_by_month.columns)
        df_seisakubutsu_filtered = pd.DataFrame(columns=df_seisakubutsu_filtered_by_month.columns)

    # --- åˆ¶ä½œç‰©åãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ---
    st.sidebar.subheader("åˆ¶ä½œç‰©åãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    name_filter_text = st.sidebar.text_input('åˆ¶ä½œç‰©åã«å«ã¾ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã§çµã‚Šè¾¼ã¿')
    if name_filter_text:
        df_filtered = df_filtered[df_filtered['åˆ¶ä½œç‰©å'].str.contains(name_filter_text, na=False)].copy()
        df_seisakubutsu_filtered = df_seisakubutsu_filtered[df_seisakubutsu_filtered['åˆ¶ä½œç‰©å'].str.contains(name_filter_text, na=False)].copy()

    # --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ ---
    st.sidebar.header("3. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    if not df_filtered.empty:
        output = io.BytesIO()
        df_filtered.to_csv(output, index=False, encoding='shift_jis', errors='ignore')
        st.sidebar.text("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®çµ±åˆCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        st.sidebar.download_button(
            label="â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", 
            data=output.getvalue(), 
            file_name='filtered_data.csv', 
            mime='text/csv'
        )


    # --- ãƒ¡ã‚¤ãƒ³ã®è¡¨ç¤ºã‚¨ãƒªã‚¢ ---
    if df_filtered.empty:
        st.warning("é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    st.success(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†ã€‚ç¾åœ¨ {len(df_seisakubutsu_filtered.drop_duplicates(subset=['ãƒˆãƒ¼ã‚¯ãƒ³']))} ä»¶ã®åˆ¶ä½œãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­ã§ã™ã€‚")

    # --- ã‚¿ãƒ–ã«è¡¨ç¤ºã™ã‚‹å·¥ç¨‹ã‚’æ±ºå®š ---
    processes_for_tabs = [p for p in original_process_order if p in df_filtered['å·¥ç¨‹'].unique()]

    # --- å·¥ç¨‹åˆ¥ çµ±åˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ---
    st.text("")
    st.header("ğŸ“Š å·¥ç¨‹åˆ¥ çµ±åˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.markdown("å„å·¥ç¨‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨è©³ç´°åˆ†æã‚’ã€ä»¥ä¸‹ã®ã‚¿ãƒ–ã§åˆ‡ã‚Šæ›¿ãˆã¦ç¢ºèªã§ãã¾ã™ã€‚")

    # --- ã‚¿ãƒ–è¡¨ç¤ºã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ ---
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…ˆã«è¨ˆç®—
    df_performance = pd.DataFrame() # Initialize empty
    if not df_filtered.empty and not relevant_grades.empty:
        # df_filtered ã«å­¦å¹´ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
        df_filtered_with_grade = pd.merge(df_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left')
        df_seisakubutsu_filtered_with_grade = pd.merge(df_seisakubutsu_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left')

        # 1. åˆ†æå¯¾è±¡ã®å­¦å¹´ã¨å·¥ç¨‹ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        active_processes = processes_for_tabs
        if selected_grades and active_processes:
            # 2. éª¨æ ¼ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
            scaffold_mux = pd.MultiIndex.from_product([selected_grades, active_processes], names=['å­¦å¹´', 'å·¥ç¨‹'])
            scaffold_df = pd.DataFrame(index=scaffold_mux).reset_index()

            # 3. å„æŒ‡æ¨™ã‚’è¨ˆç®—
            # ç·å·¥ç¨‹æ•°
            total_process_count_df = df_seisakubutsu_filtered_with_grade.groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size().rename('ç·å·¥ç¨‹æ•°').reset_index()

            # æœŸé™å†…å®Œäº†ç‡
            completed_checks_grouped = df_filtered_with_grade[df_filtered_with_grade['ãƒã‚§ãƒƒã‚¯æ¸ˆã¿'] == True].groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size().rename('completed_count')
            on_time_checks_grouped = df_filtered_with_grade[(df_filtered_with_grade['ãƒã‚§ãƒƒã‚¯æ¸ˆã¿'] == True) & (df_filtered_with_grade['ä¿®æ­£æ—¥_header'] <= df_filtered_with_grade['ç· ã‚åˆ‡ã‚Šæ—¥'])].groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size().rename('on_time_count')
            on_time_rate_df = pd.concat([completed_checks_grouped, on_time_checks_grouped], axis=1).reset_index()
            if 'completed_count' in on_time_rate_df.columns:
                on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'] = (on_time_rate_df['on_time_count'] / on_time_rate_df['completed_count']) * 100
            else:
                on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'] = 0
            on_time_rate_df.replace([float('inf'), -float('inf')], 0, inplace=True)
            on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'] = on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'].fillna(0)

            # å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•° (åˆ¶ä½œç‰©ã”ã¨(ãƒˆãƒ¼ã‚¯ãƒ³)ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ã—ãŸä¸Šã§å¹³å‡ã‚’è¨ˆç®—)
            avg_checkers_base = df_filtered_with_grade[['å­¦å¹´', 'å·¥ç¨‹', 'ãƒˆãƒ¼ã‚¯ãƒ³', 'ãƒã‚§ãƒƒã‚¯è€…æ•°']].drop_duplicates()
            avg_checkers_df = avg_checkers_base.groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True)['ãƒã‚§ãƒƒã‚¯è€…æ•°'].mean().rename('å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)').reset_index()

            # 4. éª¨æ ¼ã«ãƒãƒ¼ã‚¸
            df_performance = pd.merge(scaffold_df, total_process_count_df, on=['å­¦å¹´', 'å·¥ç¨‹'], how='left')
            df_performance = pd.merge(df_performance, on_time_rate_df[['å­¦å¹´', 'å·¥ç¨‹', 'æœŸé™å†…å®Œäº†ç‡(%)']], on=['å­¦å¹´', 'å·¥ç¨‹'], how='left')
            df_performance = pd.merge(df_performance, avg_checkers_df, on=['å­¦å¹´', 'å·¥ç¨‹'], how='left')

            # 5. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            numeric_cols = ['ç·å·¥ç¨‹æ•°', 'æœŸé™å†…å®Œäº†ç‡(%)', 'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)']
            for col in numeric_cols:
                if col in df_performance.columns:
                    df_performance[col] = df_performance[col].fillna(0)
            
            if 'ç·å·¥ç¨‹æ•°' in df_performance.columns:
                df_performance['ç·å·¥ç¨‹æ•°'] = df_performance['ç·å·¥ç¨‹æ•°'].astype(int)

            df_performance = df_performance.round(1)
            
            # è¡¨ç¤ºç”¨ã«ã‚½ãƒ¼ãƒˆ
            df_performance['å­¦å¹´'] = pd.Categorical(df_performance['å­¦å¹´'], categories=selected_grades, ordered=True)
            df_performance['å·¥ç¨‹'] = pd.Categorical(df_performance['å·¥ç¨‹'], categories=active_processes, ordered=True)
            df_performance = df_performance.sort_values(by=['å­¦å¹´', 'å·¥ç¨‹'])

    # --- çµ±åˆã‚¿ãƒ–ã®ä½œæˆ ---
    if processes_for_tabs:
        process_tabs = st.tabs(processes_for_tabs)

        for i, process_name in enumerate(processes_for_tabs):
            with process_tabs[i]:
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ç¾åœ¨ã®ã‚¿ãƒ–ã®å·¥ç¨‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                df_process_detail_filtered = df_filtered[df_filtered['å·¥ç¨‹'] == process_name].copy()
                df_seisakubutsu_process_detail_filtered = df_seisakubutsu_filtered[df_seisakubutsu_filtered['å·¥ç¨‹'] == process_name].copy()

                if df_process_detail_filtered.empty:
                    st.info(f"ã“ã®å·¥ç¨‹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                    continue

                # --- Part 1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ ---
                st.subheader(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚­ãƒ³ã‚°")
                if not df_performance.empty:
                    df_performance_tab = df_performance[df_performance['å·¥ç¨‹'] == process_name].copy()
                    if not df_performance_tab.empty:
                        st.dataframe(df_performance_tab[['å­¦å¹´', 'ç·å·¥ç¨‹æ•°', 'æœŸé™å†…å®Œäº†ç‡(%)', 'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)']], use_container_width=True)
                    else:
                        st.info(f"{process_name} ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    st.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                

                st.markdown("---")

                # --- Part 2: è©³ç´°åˆ†æ ---
                st.subheader(f"è©³ç´°åˆ†æ")

                # --- å·¥ç¨‹ãƒ»å“è³ªåˆ†æ --- 
                rework_definitions = [p for p in original_process_order if
                                    ('å†æ ¡' in p or 'å¿µæ ¡' in p or 'è‰²æ ¡' in p or 'Î±' in p or 'Î²' in p) and 
                                    p in df_seisakubutsu_process_detail_filtered['å·¥ç¨‹'].unique()]
                df_seisakubutsu_process_detail_filtered['æ‰‹æˆ»ã‚Š'] = df_seisakubutsu_process_detail_filtered['å·¥ç¨‹'].isin(rework_definitions)
                
                # å­¦å¹´ãƒ»å·¥ç¨‹åˆ¥ã®æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—çŠ¶æ³
                st.markdown("**å­¦å¹´åˆ¥ã®ã€æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—ã€çŠ¶æ³**")
                
                melted_grades_current_process = df_seisakubutsu_process_detail_filtered.melt(id_vars=['ãƒˆãƒ¼ã‚¯ãƒ³'], value_vars=grade_cols, var_name='å­¦å¹´', value_name='å¯¾è±¡')
                relevant_grades_current_process = melted_grades_current_process[melted_grades_current_process['å¯¾è±¡'] == True]

                df_next_check_base = pd.merge(df_process_detail_filtered, relevant_grades_current_process[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left')

                scaffold_grades_df = pd.DataFrame({'å­¦å¹´': selected_grades})

                if not df_next_check_base.empty and 'å­¦å¹´' in df_next_check_base.columns:
                    grouped = df_next_check_base.groupby(['å­¦å¹´'], observed=True)['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—']
                    count = grouped.sum().astype(int).rename('æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°')
                    total_in_group = grouped.size()
                    ratio = (count / total_in_group * 100).round(1).rename('æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)')
                    calculated_df = pd.concat([count, ratio], axis=1).reset_index()
                else:
                    calculated_df = pd.DataFrame(columns=['å­¦å¹´', 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°', 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)'])

                result_df = pd.merge(scaffold_grades_df, calculated_df, on='å­¦å¹´', how='left')
                result_df[['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°', 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)']] = result_df[['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°', 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)']].fillna(0)
                result_df['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°'] = result_df['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°'].astype(int)
                result_df['å­¦å¹´'] = pd.Categorical(result_df['å­¦å¹´'], categories=selected_grades, ordered=True)
                result_df = result_df.sort_values(by='å­¦å¹´')

                if not result_df.empty:
                    fig_ratio = px.bar(result_df, x='å­¦å¹´', y='æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)', title='å­¦å¹´åˆ¥ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦ã€ã®å‰²åˆ', text_auto='.1f', color='å­¦å¹´')
                    fig_ratio.update_layout(showlegend=False, xaxis_title=None)
                    st.plotly_chart(fig_ratio, use_container_width=True, key=f"ratio_chart_{process_name}")
                    
                    fig_count = px.bar(result_df, x='å­¦å¹´', y='æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°', title='å­¦å¹´åˆ¥ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦ã€ã®äººæ•°', text_auto=True, color='å­¦å¹´')
                    fig_count.update_layout(showlegend=False, xaxis_title=None)
                    st.plotly_chart(fig_count, use_container_width=True, key=f"count_chart_{process_name}")
                else:
                    st.info(f"ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—ã€ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.info("é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«è©²å½“ã™ã‚‹å·¥ç¨‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    # --- ROIåˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæœ€ä¸‹éƒ¨ã¸ç§»å‹•ï¼‰ ---
    st.header("ROIï¼ˆè²»ç”¨å¯¾åŠ¹æœï¼‰åˆ†æ")
    with st.expander("ç·çŸ­ç¸®æ™‚é–“ã®è©¦ç®—"):
        total_requests = len(df_seisakubutsu_filtered['ãƒˆãƒ¼ã‚¯ãƒ³'].unique())
        time_saved_per_request = 5  # 1ä»¶ã‚ãŸã‚Šã®çŸ­ç¸®æ™‚é–“ï¼ˆåˆ†ï¼‰
        total_minutes_saved = total_requests * time_saved_per_request
        hours_saved = total_minutes_saved // 60
        minutes_saved = total_minutes_saved % 60

        st.metric(
            label=f"æœŸé–“å†…ã®ç·ãƒã‚§ãƒƒã‚¯å‡ºã—æ¡ˆä»¶æ•°",
            value=f"{total_requests} ä»¶",
            delta=f"ç´„ {hours_saved} æ™‚é–“ {minutes_saved} åˆ†ã®çŸ­ç¸®åŠ¹æœãŒè¦‹è¾¼ã¾ã‚Œã¾ã™",
            delta_color="off"
        )
        st.info(f"ã“ã®è©¦ç®—ã¯ã€1æ¡ˆä»¶ã‚ãŸã‚Š {time_saved_per_request} åˆ†ã®æ™‚é–“ãŒçŸ­ç¸®ã•ã‚Œã‚‹ã¨ä»®å®šã—ãŸå ´åˆã®ã‚‚ã®ã§ã™ã€‚")

else:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ†æå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’2ã¤ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€åˆ†æãŒå§‹ã¾ã‚Šã¾ã™ã€‚")

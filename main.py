# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import plotly.express as px
import io
import gc
from datetime import datetime

# =========================
# å®‰å…¨é‹ç”¨ãƒ¡ãƒ¢
# - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœªä½¿ç”¨ï¼ˆ@st.cache_dataå‰Šé™¤ï¼‰
# - PII(æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹)ã¯é›†è¨ˆå¾Œã«å³drop
# - ä¾‹å¤–ã¯ç°¡ç´ åŒ–ã—ã¦è©³ç´°ã¯å‡ºã•ãªã„
# - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯UTF-8-SIGï¼ˆæ–‡å­—åŒ–ã‘&æ¬ è½å›é¿ï¼‰
# - ãƒ­ã‚°(st.write/print)ã«ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å‡ºã•ãªã„
# =========================

month_order = ['4æœˆå·', '5æœˆå·', '6æœˆå·', '7æœˆå·', '8æœˆå·', '9æœˆå·', '10æœˆå·', '11æœˆå·', '12æœˆå·', '1æœˆå·', '2æœˆå·', '3æœˆå·', 'ãã®ä»–']

# å·¥ç¨‹ã®é †åºï¼ˆé‡è¤‡é™¤å»æ¸ˆï¼‰
original_process_order = list(dict.fromkeys([
    'ä»®å°å‰²', 'å…¥ç¨¿å‰ãƒ©ãƒ•', 'å…¥ç¨¿åŸç¨¿', 'çµ„ç‰ˆåŸç¨¿', 'åˆæ ¡', 'å†æ ¡', 'å†æ ¡2', 'å†æ ¡3',
    'è‰²æ ¡', 'è‰²æ ¡2', 'è‰²æ ¡3', 'å¿µæ ¡', 'å¿µæ ¡2', 'å¿µæ ¡3', 'Î±1ç‰ˆ', 'Î²1ç‰ˆ', 'Î²2ç‰ˆ',
    'Î²3ç‰ˆ', 'Î²4ç‰ˆ', 'Î²5ç‰ˆ', 'ãã®ä»–'
]))

# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="ç¤¾å†…ãƒã‚§ãƒƒã‚¯æ¥­å‹™ BPRåˆ†æãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ“Š ç¤¾å†…ãƒã‚§ãƒƒã‚¯æ¥­å‹™ BPRåˆ†æãƒ„ãƒ¼ãƒ«")

# --- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ ---
st.sidebar.header("1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
st.sidebar.info("åˆ†æå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’2ã¤ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
uploaded_seisakubutsu_file = st.sidebar.file_uploader("åˆ¶ä½œç‰©ä¸€è¦§ CSV", type="csv")
uploaded_header_file = st.sidebar.file_uploader("ãƒ˜ãƒƒãƒ€ãƒ¼ä¸€è¦§ CSV", type="csv")

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆéã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ ---
def load_data(seisakubutsu_file, header_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰CSVã‚’èª­ã¿è¾¼ã¿ã€çµåˆãƒ»å‰å‡¦ç†ã‚’è¡Œã†ï¼ˆPIIã¯æ—©æœŸé™¤å»ï¼‰"""
    try:
        seisakubutsu_df = pd.read_csv(seisakubutsu_file)
        header_df = pd.read_csv(header_file)
    except Exception:
        st.error("ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚CSVå½¢å¼ã‚„æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        return None, None

    # æ—¥ä»˜åˆ—ã®æ­£è¦åŒ–ï¼ˆtzç„¡ã—ï¼‰
    date_cols = ['ä½œæˆæ—¥', 'ä¿®æ­£æ—¥', 'ç· ã‚åˆ‡ã‚Šæ—¥']
    for col in date_cols:
        if col in seisakubutsu_df.columns:
            seisakubutsu_df[col] = pd.to_datetime(seisakubutsu_df[col], errors='coerce').dt.tz_localize(None)
        if col in header_df.columns:
            header_df[col] = pd.to_datetime(header_df[col], errors='coerce').dt.tz_localize(None)

    # åˆ—åæ•´å½¢ & ãƒã‚§ãƒƒã‚¯è€…æ•°é›†è¨ˆ
    header_df.rename(columns={'åˆ¶ä½œç‰©ãƒˆãƒ¼ã‚¯ãƒ³': 'ãƒˆãƒ¼ã‚¯ãƒ³'}, inplace=True)
    if 'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in header_df.columns:
        checkers_count_df = header_df.groupby('ãƒˆãƒ¼ã‚¯ãƒ³')['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].nunique().reset_index()
        checkers_count_df.rename(columns={'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒã‚§ãƒƒã‚¯è€…æ•°'}, inplace=True)
        # PIIã¯ä»¥é™ä¸è¦ãªã®ã§é€Ÿã‚„ã‹ã«å‰Šé™¤
        header_df = header_df.drop(columns=['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'])
    else:
        checkers_count_df = pd.DataFrame(columns=['ãƒˆãƒ¼ã‚¯ãƒ³', 'ãƒã‚§ãƒƒã‚¯è€…æ•°'])

    # åˆ¶ä½œç‰©å´ã¸ãƒã‚§ãƒƒã‚¯è€…æ•°ã‚’ä»˜ä¸
    seisakubutsu_df = pd.merge(seisakubutsu_df, checkers_count_df, on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left')
    seisakubutsu_df['ãƒã‚§ãƒƒã‚¯è€…æ•°'] = seisakubutsu_df['ãƒã‚§ãƒƒã‚¯è€…æ•°'].fillna(0)

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨åˆ¶ä½œç‰©ã‚’çµåˆ
    merged_df = pd.merge(header_df, seisakubutsu_df, on='ãƒˆãƒ¼ã‚¯ãƒ³', suffixes=('_header', '_seisakubutsu'))

    # ç™ºåˆŠæœˆã®ã‚«ãƒ†ã‚´ãƒªé †åº
    for df in (merged_df, seisakubutsu_df):
        if 'ç™ºåˆŠæœˆ' in df.columns:
            df['ç™ºåˆŠæœˆ'] = pd.Categorical(df['ç™ºåˆŠæœˆ'], categories=month_order, ordered=True)

    # å®Ÿãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹å·¥ç¨‹ã®ã¿è¨±å®¹
    existing_processes = pd.concat([
        merged_df['å·¥ç¨‹'] if 'å·¥ç¨‹' in merged_df.columns else pd.Series(dtype=str),
        seisakubutsu_df['å·¥ç¨‹'] if 'å·¥ç¨‹' in seisakubutsu_df.columns else pd.Series(dtype=str)
    ]).dropna().unique()
    filtered_process_order = [p for p in original_process_order if p in existing_processes]
    for df in (merged_df, seisakubutsu_df):
        if 'å·¥ç¨‹' in df.columns:
            df['å·¥ç¨‹'] = pd.Categorical(df['å·¥ç¨‹'], categories=filtered_process_order, ordered=True)

    return merged_df, seisakubutsu_df

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if uploaded_seisakubutsu_file is not None and uploaded_header_file is not None:
    df_merged_all, df_seisakubutsu_all = load_data(uploaded_seisakubutsu_file, uploaded_header_file)
    if df_merged_all is None:
        st.stop()

    st.sidebar.header("2. çµã‚Šè¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    if 'ä½œæˆæ—¥' not in df_seisakubutsu_all.columns or df_seisakubutsu_all['ä½œæˆæ—¥'].dropna().empty:
        st.error("ã‚¨ãƒ©ãƒ¼: åˆ¶ä½œç‰©CSVã«ã€ä½œæˆæ—¥ã€åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã¾ãŸã¯å…¨ã¦æ¬ æï¼‰")
        st.stop()

    min_date = df_seisakubutsu_all['ä½œæˆæ—¥'].min().date()
    max_date = df_seisakubutsu_all['ä½œæˆæ—¥'].max().date()
    start_date = st.sidebar.date_input('é–‹å§‹æ—¥', min_date, min_value=min_date, max_value=max_date)
    end_date = st.sidebar.date_input('çµ‚äº†æ—¥', max_date, min_value=start_date, max_value=max_date)

    if start_date > end_date:
        st.sidebar.error('ã‚¨ãƒ©ãƒ¼: çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚')
        st.stop()

    start_datetime = pd.to_datetime(start_date)
    end_datetime = pd.to_datetime(end_date) + pd.Timedelta(days=1)

    # æœŸé–“é©ç”¨ï¼ˆçµåˆå´ã¯ä½œæˆæ—¥_seisakubutsuï¼‰
    df_filtered_by_date = df_merged_all[
        (df_merged_all['ä½œæˆæ—¥_seisakubutsu'] >= start_datetime) &
        (df_merged_all['ä½œæˆæ—¥_seisakubutsu'] < end_datetime)
    ].copy()
    df_seisakubutsu_filtered_by_date = df_seisakubutsu_all[
        (df_seisakubutsu_all['ä½œæˆæ—¥'] >= start_datetime) &
        (df_seisakubutsu_all['ä½œæˆæ—¥'] < end_datetime)
    ].copy()

    # ç™ºåˆŠå¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.subheader("ç™ºåˆŠå¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    available_years = sorted(df_seisakubutsu_filtered_by_date['å¹´åº¦'].dropna().unique().tolist()) \
        if 'å¹´åº¦' in df_seisakubutsu_filtered_by_date.columns else []
    selected_year = st.sidebar.selectbox('æ¯”è¼ƒã—ãŸã„ç™ºåˆŠå¹´åº¦ã‚’é¸æŠ', options=['ã™ã¹ã¦'] + available_years)

    # ç™ºåˆŠæœˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.subheader("ç™ºåˆŠæœˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    if 'ç™ºåˆŠæœˆ' in df_seisakubutsu_filtered_by_date.columns:
        available_months = [m for m in month_order if m in df_seisakubutsu_filtered_by_date['ç™ºåˆŠæœˆ'].dropna().unique()]
    else:
        available_months = []
    selected_month = st.sidebar.selectbox('æ¯”è¼ƒã—ãŸã„ç™ºåˆŠæœˆã‚’é¸æŠ', options=['ã™ã¹ã¦'] + available_months)

    df_filtered_by_month = df_filtered_by_date.copy()
    df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_date.copy()
    if selected_year != 'ã™ã¹ã¦' and 'å¹´åº¦' in df_filtered_by_month.columns:
        df_filtered_by_month = df_filtered_by_month[df_filtered_by_month['å¹´åº¦'] == selected_year]
        df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['å¹´åº¦'] == selected_year]
    if selected_month != 'ã™ã¹ã¦' and 'ç™ºåˆŠæœˆ' in df_filtered_by_month.columns:
        df_filtered_by_month = df_filtered_by_month[df_filtered_by_month['ç™ºåˆŠæœˆ'] == selected_month]
        df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['ç™ºåˆŠæœˆ'] == selected_month]

    # å­¦å¹´ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.subheader("å­¦å¹´ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    grade_cols = [c for c in df_seisakubutsu_all.columns if ('å¹´ç”Ÿ' in c or 'å­¦å¹´ãã®ä»–' in c)]
    melted_grades = df_seisakubutsu_filtered_by_month.melt(
        id_vars=['ãƒˆãƒ¼ã‚¯ãƒ³'], value_vars=grade_cols, var_name='å­¦å¹´', value_name='å¯¾è±¡'
    ) if grade_cols else pd.DataFrame(columns=['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´', 'å¯¾è±¡'])

    relevant_grades = melted_grades[melted_grades['å¯¾è±¡'] == True] if not melted_grades.empty else pd.DataFrame(columns=['ãƒˆãƒ¼ã‚¯ãƒ³','å­¦å¹´'])
    available_grades = relevant_grades['å­¦å¹´'].unique().tolist() if not relevant_grades.empty else []
    selected_grades = st.sidebar.multiselect('åˆ†æã—ãŸã„å­¦å¹´ã‚’é¸æŠ', options=available_grades, default=available_grades)

    if selected_grades and not relevant_grades.empty:
        selected_tokens = relevant_grades[relevant_grades['å­¦å¹´'].isin(selected_grades)]['ãƒˆãƒ¼ã‚¯ãƒ³'].unique()
        df_filtered = df_filtered_by_month[df_filtered_by_month['ãƒˆãƒ¼ã‚¯ãƒ³'].isin(selected_tokens)].copy()
        df_seisakubutsu_filtered = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['ãƒˆãƒ¼ã‚¯ãƒ³'].isin(selected_tokens)].copy()
    else:
        df_filtered = pd.DataFrame(columns=df_filtered_by_month.columns)
        df_seisakubutsu_filtered = pd.DataFrame(columns=df_seisakubutsu_filtered_by_month.columns)

    # åˆ¶ä½œç‰©åãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.subheader("åˆ¶ä½œç‰©åãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    name_filter_text = st.sidebar.text_input('åˆ¶ä½œç‰©åã«å«ã¾ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã§çµã‚Šè¾¼ã¿')
    if name_filter_text and 'åˆ¶ä½œç‰©å' in df_filtered.columns:
        df_filtered = df_filtered[df_filtered['åˆ¶ä½œç‰©å'].str.contains(name_filter_text, na=False)].copy()
        df_seisakubutsu_filtered = df_seisakubutsu_filtered[df_seisakubutsu_filtered['åˆ¶ä½œç‰©å'].str.contains(name_filter_text, na=False)].copy()

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    st.sidebar.header("3. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    if not df_filtered.empty:
        output = io.BytesIO()
        # æ–‡å­—æ¬ è½ã‚’é¿ã‘ã‚‹ãŸã‚ UTF-8-SIG ã‚’ä½¿ç”¨ï¼ˆExceläº’æ›ï¼‰
        df_filtered.to_csv(output, index=False, encoding='utf-8-sig')
        st.sidebar.text("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®çµ±åˆCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        st.sidebar.download_button(
            label="â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=output.getvalue(),
            file_name='filtered_data.csv',
            mime='text/csv'
        )
        del output

    # è¡¨ç¤ºã‚¬ãƒ¼ãƒ‰
    if df_filtered.empty:
        st.warning("é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ã‚µãƒãƒªãƒ¼
    unique_items = df_seisakubutsu_filtered['åˆ¶ä½œç‰©å'].nunique() if 'åˆ¶ä½œç‰©å' in df_seisakubutsu_filtered.columns else 0
    st.success(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†ã€‚ç¾åœ¨ {unique_items} ä»¶ã®åˆ¶ä½œç‰©ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­ã§ã™ã€‚")

    processes_for_tabs = [p for p in original_process_order if 'å·¥ç¨‹' in df_filtered.columns and p in df_filtered['å·¥ç¨‹'].unique()]

    # --- å…¨ä½“ã‚µãƒãƒªãƒ¼ ---
    st.text("")
    st.header("ğŸ“Š å…¨ä½“ã‚µãƒãƒªãƒ¼")
    st.markdown("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§çµã‚Šè¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®æ¦‚è¦ï¼ˆå­¦å¹´åˆ¥ãƒ»åˆè¨ˆï¼‰ã¨ã€ç™ºåˆŠæœˆã”ã¨ã®æ¨ç§»ã‚’ç¢ºèªã§ãã¾ã™ã€‚")

    # å­¦å¹´åˆ¥ã‚µãƒãƒªãƒ¼
    if not relevant_grades.empty:
        df_filtered_with_grade_summary = pd.merge(
            df_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left'
        )
        df_seisakubutsu_with_grade_summary = pd.merge(
            df_seisakubutsu_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left'
        )

        summary_items = df_seisakubutsu_with_grade_summary.groupby('å­¦å¹´', observed=True)['åˆ¶ä½œç‰©å'].nunique().rename('ç·åˆ¶ä½œç‰©ä»¶æ•°')
        summary_processes = df_seisakubutsu_with_grade_summary.groupby('å­¦å¹´', observed=True).size().rename('ç·å·¥ç¨‹æ•°')

        completed_checks = df_filtered_with_grade_summary[df_filtered_with_grade_summary.get('ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', False) == True] \
            .groupby('å­¦å¹´', observed=True).size().rename('completed')
        on_time_checks = df_filtered_with_grade_summary[
            (df_filtered_with_grade_summary.get('ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', False) == True) &
            (df_filtered_with_grade_summary.get('ä¿®æ­£æ—¥_header') <= df_filtered_with_grade_summary.get('ç· ã‚åˆ‡ã‚Šæ—¥'))
        ].groupby('å­¦å¹´', observed=True).size().rename('on_time')

        on_time_summary = pd.concat([completed_checks, on_time_checks], axis=1).fillna(0)
        on_time_summary['æœŸé™å†…å®Œäº†ç‡(%)'] = (
            on_time_summary['on_time'] / on_time_summary['completed'] * 100
        ).where(on_time_summary['completed'] > 0, 0)

        avg_checkers = df_seisakubutsu_with_grade_summary.drop_duplicates(subset=['å­¦å¹´', 'ãƒˆãƒ¼ã‚¯ãƒ³']) \
            .groupby('å­¦å¹´', observed=True)['ãƒã‚§ãƒƒã‚¯è€…æ•°'].mean().rename('å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)')

        summary_by_grade_df = pd.concat(
            [summary_items, summary_processes, on_time_summary['æœŸé™å†…å®Œäº†ç‡(%)'], avg_checkers], axis=1
        ).reset_index().fillna(0)

        # å…¨ä½“åˆè¨ˆ
        total_items = df_seisakubutsu_filtered['åˆ¶ä½œç‰©å'].nunique() if 'åˆ¶ä½œç‰©å' in df_seisakubutsu_filtered.columns else 0
        total_processes = len(df_seisakubutsu_filtered)
        total_completed = int(df_filtered.get('ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', pd.Series(dtype=bool)).sum()) if 'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿' in df_filtered.columns else 0
        total_on_time = df_filtered[
            (df_filtered.get('ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', False) == True) &
            (df_filtered.get('ä¿®æ­£æ—¥_header') <= df_filtered.get('ç· ã‚åˆ‡ã‚Šæ—¥'))
        ].shape[0] if {'ä¿®æ­£æ—¥_header','ç· ã‚åˆ‡ã‚Šæ—¥','ãƒã‚§ãƒƒã‚¯æ¸ˆã¿'}.issubset(df_filtered.columns) else 0
        total_on_time_rate = (total_on_time / total_completed * 100) if total_completed > 0 else 0
        total_avg_checkers = df_seisakubutsu_filtered.drop_duplicates(subset=['ãƒˆãƒ¼ã‚¯ãƒ³'])['ãƒã‚§ãƒƒã‚¯è€…æ•°'].mean() \
            if 'ãƒã‚§ãƒƒã‚¯è€…æ•°' in df_seisakubutsu_filtered.columns else 0

        total_summary_df = pd.DataFrame([{
            'å­¦å¹´': 'åˆè¨ˆ',
            'ç·åˆ¶ä½œç‰©ä»¶æ•°': total_items,
            'ç·å·¥ç¨‹æ•°': total_processes,
            'æœŸé™å†…å®Œäº†ç‡(%)': total_on_time_rate,
            'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)': total_avg_checkers
        }])

        final_summary_df = pd.concat([summary_by_grade_df, total_summary_df], ignore_index=True).fillna(0)
        for col in ['ç·åˆ¶ä½œç‰©ä»¶æ•°', 'ç·å·¥ç¨‹æ•°']:
            if col in final_summary_df.columns:
                final_summary_df[col] = final_summary_df[col].astype(int)
        final_summary_df = final_summary_df.round(1)

        st.subheader("å­¦å¹´åˆ¥ã‚µãƒãƒªãƒ¼")
        show_cols = ['å­¦å¹´', 'ç·åˆ¶ä½œç‰©ä»¶æ•°', 'ç·å·¥ç¨‹æ•°', 'æœŸé™å†…å®Œäº†ç‡(%)', 'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)']
        st.dataframe(final_summary_df[[c for c in show_cols if c in final_summary_df.columns]], use_container_width=True)
    else:
        st.info("å­¦å¹´ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€å­¦å¹´åˆ¥ã‚µãƒãƒªãƒ¼ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    st.markdown("---")

    # ç™ºåˆŠæœˆã®æ¨ç§»
    st.subheader("ç™ºåˆŠæœˆã”ã¨ã®æ¨ç§»")
    if not relevant_grades.empty and 'ç™ºåˆŠæœˆ' in df_seisakubutsu_filtered.columns:
        monthly_by_grade = df_seisakubutsu_with_grade_summary.groupby(['ç™ºåˆŠæœˆ', 'å­¦å¹´'], observed=True).agg(
            åˆ¶ä½œç‰©ä»¶æ•°=('åˆ¶ä½œç‰©å', 'nunique'),
            ç·å·¥ç¨‹æ•°=('ãƒˆãƒ¼ã‚¯ãƒ³', 'size')
        ).reset_index()

        monthly_total = df_seisakubutsu_filtered.groupby('ç™ºåˆŠæœˆ', observed=True).agg(
            åˆ¶ä½œç‰©ä»¶æ•°=('åˆ¶ä½œç‰©å', 'nunique'),
            ç·å·¥ç¨‹æ•°=('ãƒˆãƒ¼ã‚¯ãƒ³', 'size')
        ).reset_index()
        monthly_total['å­¦å¹´'] = 'åˆè¨ˆ'

        monthly_summary = pd.concat([monthly_by_grade, monthly_total], ignore_index=True)
        monthly_summary['ç™ºåˆŠæœˆ'] = pd.Categorical(monthly_summary['ç™ºåˆŠæœˆ'], categories=month_order, ordered=True)
        monthly_summary_for_graph = monthly_summary[monthly_summary['ç™ºåˆŠæœˆ'] != 'ãã®ä»–'].copy().sort_values('ç™ºåˆŠæœˆ')

        if not monthly_summary_for_graph.empty:
            col1, col2 = st.columns(2)
            with col1:
                fig_a = px.line(monthly_summary_for_graph, x='ç™ºåˆŠæœˆ', y='åˆ¶ä½œç‰©ä»¶æ•°', color='å­¦å¹´',
                                title='ç™ºåˆŠæœˆã”ã¨ã®åˆ¶ä½œç‰©ä»¶æ•°', markers=True,
                                color_discrete_sequence=px.colors.qualitative.T10)
                fig_a.update_layout(xaxis_title=None, legend_title_text='å­¦å¹´')
                fig_a.update_xaxes(categoryorder='array', categoryarray=[m for m in month_order if m != 'ãã®ä»–'])
                st.plotly_chart(fig_a, use_container_width=True)
            with col2:
                fig_b = px.line(monthly_summary_for_graph, x='ç™ºåˆŠæœˆ', y='ç·å·¥ç¨‹æ•°', color='å­¦å¹´',
                                title='ç™ºåˆŠæœˆã”ã¨ã®ç·å·¥ç¨‹æ•°', markers=True,
                                color_discrete_sequence=px.colors.qualitative.T10)
                fig_b.update_layout(xaxis_title=None, legend_title_text='å­¦å¹´')
                fig_b.update_xaxes(categoryorder='array', categoryarray=[m for m in month_order if m != 'ãã®ä»–'])
                st.plotly_chart(fig_b, use_container_width=True)
        else:
            st.info("ç™ºåˆŠæœˆã”ã¨ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.info("å­¦å¹´ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ç™ºåˆŠæœˆã”ã¨ã®æ¨ç§»ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    # --- å·¥ç¨‹åˆ¥ çµ±åˆåˆ†æ ---
    st.text("")
    st.header("ğŸ“Š å·¥ç¨‹åˆ¥ çµ±åˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.markdown("å„å·¥ç¨‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨è©³ç´°åˆ†æã‚’ã€ä»¥ä¸‹ã®ã‚¿ãƒ–ã§åˆ‡ã‚Šæ›¿ãˆã¦ç¢ºèªã§ãã¾ã™ã€‚")

    df_performance = pd.DataFrame()
    if not df_filtered.empty and not relevant_grades.empty and 'å·¥ç¨‹' in df_filtered.columns:
        df_filtered_with_grade = pd.merge(
            df_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left'
        )
        df_seisakubutsu_with_grade = pd.merge(
            df_seisakubutsu_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left'
        )

        active_processes = processes_for_tabs
        if selected_grades and active_processes:
            scaffold_df = pd.MultiIndex.from_product([selected_grades, active_processes], names=['å­¦å¹´', 'å·¥ç¨‹']).to_frame(index=False)

            total_process_count_df = df_seisakubutsu_with_grade.groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size() \
                .rename('ç·å·¥ç¨‹æ•°').reset_index()

            completed_g = df_filtered_with_grade[df_filtered_with_grade.get('ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', False) == True] \
                .groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size().rename('completed_count')
            ontime_g = df_filtered_with_grade[
                (df_filtered_with_grade.get('ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', False) == True) &
                (df_filtered_with_grade.get('ä¿®æ­£æ—¥_header') <= df_filtered_with_grade.get('ç· ã‚åˆ‡ã‚Šæ—¥'))
            ].groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size().rename('on_time_count')

            on_time_rate_df = pd.concat([completed_g, ontime_g], axis=1).reset_index().fillna(0)
            on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'] = (
                on_time_rate_df['on_time_count'] / on_time_rate_df['completed_count']
            ).replace([float('inf'), -float('inf')], 0).fillna(0) * 100

            avg_checkers_df = df_filtered_with_grade[['å­¦å¹´', 'å·¥ç¨‹', 'ãƒˆãƒ¼ã‚¯ãƒ³', 'ãƒã‚§ãƒƒã‚¯è€…æ•°']] \
                .drop_duplicates().groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True)['ãƒã‚§ãƒƒã‚¯è€…æ•°'] \
                .mean().rename('å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)').reset_index()

            df_performance = scaffold_df.merge(total_process_count_df, on=['å­¦å¹´', 'å·¥ç¨‹'], how='left') \
                                        .merge(on_time_rate_df[['å­¦å¹´', 'å·¥ç¨‹', 'æœŸé™å†…å®Œäº†ç‡(%)']], on=['å­¦å¹´', 'å·¥ç¨‹'], how='left') \
                                        .merge(avg_checkers_df, on=['å­¦å¹´', 'å·¥ç¨‹'], how='left') \
                                        .fillna(0)

            if 'ç·å·¥ç¨‹æ•°' in df_performance.columns:
                df_performance['ç·å·¥ç¨‹æ•°'] = df_performance['ç·å·¥ç¨‹æ•°'].astype(int)
            df_performance = df_performance.round(1)
            df_performance['å­¦å¹´'] = pd.Categorical(df_performance['å­¦å¹´'], categories=selected_grades, ordered=True)
            df_performance['å·¥ç¨‹'] = pd.Categorical(df_performance['å·¥ç¨‹'], categories=active_processes, ordered=True)
            df_performance = df_performance.sort_values(by=['å­¦å¹´', 'å·¥ç¨‹'])

    if processes_for_tabs:
        process_tabs = st.tabs(processes_for_tabs)
        for i, process_name in enumerate(processes_for_tabs):
            with process_tabs[i]:
                df_proc = df_filtered[df_filtered.get('å·¥ç¨‹') == process_name].copy()
                df_proc_sei = df_seisakubutsu_filtered[df_seisakubutsu_filtered.get('å·¥ç¨‹') == process_name].copy()

                if df_proc.empty:
                    st.info("ã“ã®å·¥ç¨‹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                    continue

                st.subheader("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚­ãƒ³ã‚°")
                if not df_performance.empty:
                    view = df_performance[df_performance['å·¥ç¨‹'] == process_name]
                    if not view.empty:
                        st.dataframe(view[['å­¦å¹´', 'ç·å·¥ç¨‹æ•°', 'æœŸé™å†…å®Œäº†ç‡(%)', 'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)']], use_container_width=True)
                    else:
                        st.info(f"{process_name} ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    st.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

                st.markdown("---")
                st.subheader("è©³ç´°åˆ†æ")

                # æ‰‹æˆ»ã‚Šåˆ¤å®š
                rework_defs = [p for p in original_process_order if
                               ('å†æ ¡' in p or 'å¿µæ ¡' in p or 'è‰²æ ¡' in p or 'Î±' in p or 'Î²' in p)]
                if 'å·¥ç¨‹' in df_proc_sei.columns:
                    df_proc_sei['æ‰‹æˆ»ã‚Š'] = df_proc_sei['å·¥ç¨‹'].isin(rework_defs)

                st.markdown("**å­¦å¹´åˆ¥ã®ã€æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—ã€çŠ¶æ³**")
                if grade_cols:
                    melted_cur = df_proc_sei.melt(id_vars=['ãƒˆãƒ¼ã‚¯ãƒ³'], value_vars=grade_cols,
                                                  var_name='å­¦å¹´', value_name='å¯¾è±¡') if not df_proc_sei.empty else pd.DataFrame(columns=['ãƒˆãƒ¼ã‚¯ãƒ³','å­¦å¹´','å¯¾è±¡'])
                    rel_cur = melted_cur[melted_cur['å¯¾è±¡'] == True] if not melted_cur.empty else pd.DataFrame(columns=['ãƒˆãƒ¼ã‚¯ãƒ³','å­¦å¹´'])

                    df_next = pd.merge(df_proc, rel_cur[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left') \
                        if not df_proc.empty else pd.DataFrame(columns=['å­¦å¹´','æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—'])

                    scaffold_grades_df = pd.DataFrame({'å­¦å¹´': selected_grades})
                    if not df_next.empty and 'å­¦å¹´' in df_next.columns and 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—' in df_next.columns:
                        grouped = df_next.groupby(['å­¦å¹´'], observed=True)['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—']
                        count = grouped.sum().astype(int).rename('æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°')
                        total_in_group = grouped.size()
                        ratio = (count / total_in_group * 100).round(1).rename('æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)')
                        calc_df = pd.concat([count, ratio], axis=1).reset_index()
                    else:
                        calc_df = pd.DataFrame(columns=['å­¦å¹´', 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°', 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)'])

                    result_df = pd.merge(scaffold_grades_df, calc_df, on='å­¦å¹´', how='left').fillna(0)
                    result_df['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°'] = result_df['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°'].astype(int)
                    result_df['å­¦å¹´'] = pd.Categorical(result_df['å­¦å¹´'], categories=selected_grades, ordered=True)
                    result_df = result_df.sort_values(by='å­¦å¹´')

                    if not result_df.empty:
                        fig_ratio = px.bar(result_df, x='å­¦å¹´', y='æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)',
                                           title='å­¦å¹´åˆ¥ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦ã€ã®å‰²åˆ', text_auto='.1f', color='å­¦å¹´')
                        fig_ratio.update_layout(showlegend=False, xaxis_title=None)
                        st.plotly_chart(fig_ratio, use_container_width=True, key=f"ratio_chart_{process_name}")

                        fig_count = px.bar(result_df, x='å­¦å¹´', y='æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°',
                                           title='å­¦å¹´åˆ¥ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦ã€ã®äººæ•°', text_auto=True, color='å­¦å¹´')
                        fig_count.update_layout(showlegend=False, xaxis_title=None)
                        st.plotly_chart(fig_count, use_container_width=True, key=f"count_chart_{process_name}")
                    else:
                        st.info("ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—ã€ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    st.info("å­¦å¹´åˆ—ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—çŠ¶æ³ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
    else:
        st.info("é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«è©²å½“ã™ã‚‹å·¥ç¨‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    # å¾Œå‡¦ç†ï¼ˆå¤§ããªDFã‚’è§£æ”¾ï¼‰
    del df_merged_all, df_seisakubutsu_all
    del df_filtered_by_date, df_seisakubutsu_filtered_by_date
    del df_filtered_by_month, df_seisakubutsu_filtered_by_month
    del df_filtered, df_seisakubutsu_filtered
    gc.collect()

else:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ†æå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’2ã¤ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€åˆ†æãŒå§‹ã¾ã‚Šã¾ã™ã€‚")

# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import plotly.express as px
import io
import gc
from datetime import datetime

# =========================
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£/å …ç‰¢åŒ–ãƒã‚¤ãƒ³ãƒˆ
# - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœªä½¿ç”¨ï¼ˆ@st.cache_dataå‰Šé™¤ï¼‰
# - PII(æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹)ã¯é›†è¨ˆå¾Œã«å³drop
# - ä¾‹å¤–ã¯ç°¡ç´ åŒ–ã—ã¦è©³ç´°ã¯å‡ºã•ãªã„
# - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯UTF-8-SIGï¼ˆExceläº’æ›ï¼‰
# - fillnaã¯æ•°å€¤åˆ—ã®ã¿ã«é™å®š
# - æ¬ æåˆ—ã¯å®‰å…¨ã«ã‚¹ã‚­ãƒƒãƒ— or 0æ‰±ã„
# =========================

month_order = ['4æœˆå·', '5æœˆå·', '6æœˆå·', '7æœˆå·', '8æœˆå·', '9æœˆå·', '10æœˆå·',
               '11æœˆå·', '12æœˆå·', '1æœˆå·', '2æœˆå·', '3æœˆå·', 'ãã®ä»–']

original_process_order = list(dict.fromkeys([
    'ä»®å°å‰²', 'å…¥ç¨¿å‰ãƒ©ãƒ•', 'å…¥ç¨¿åŸç¨¿', 'çµ„ç‰ˆåŸç¨¿', 'åˆæ ¡', 'å†æ ¡', 'å†æ ¡2', 'å†æ ¡3',
    'è‰²æ ¡', 'è‰²æ ¡2', 'è‰²æ ¡3', 'å¿µæ ¡', 'å¿µæ ¡2', 'å¿µæ ¡3', 'Î±1ç‰ˆ',
    'Î²1ç‰ˆ', 'Î²2ç‰ˆ', 'Î²3ç‰ˆ', 'Î²4ç‰ˆ', 'Î²5ç‰ˆ', 'ãã®ä»–'
]))

# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="ç¤¾å†…ãƒã‚§ãƒƒã‚¯æ¥­å‹™ BPRåˆ†æãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ“Š ç¤¾å†…ãƒã‚§ãƒƒã‚¯æ¥­å‹™ BPRåˆ†æãƒ„ãƒ¼ãƒ«")

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
def num_fillna_inplace(df: pd.DataFrame, val=0):
    """DataFrameã®æ•°å€¤åˆ—ã ã‘NAã‚’åŸ‹ã‚ã‚‹"""
    if df is None or df.empty:
        return
    num_cols = df.select_dtypes(include=['number']).columns
    if len(num_cols) > 0:
        df[num_cols] = df[num_cols].fillna(val)

def safe_bool_series(df: pd.DataFrame, col: str) -> pd.Series:
    """df[col]==True ã‚’å®‰å…¨ã«è¿”ã™ã€‚åˆ—ãŒç„¡ã„å ´åˆã¯å…¨Falseã®Seriesã‚’è¿”ã™"""
    if df is None or df.empty:
        return pd.Series([], dtype=bool)
    if col in df.columns:
        return df[col] == True
    # å…¨Falseã®ãƒ–ãƒ¼ãƒ«Seriesï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒãˆï¼‰
    return pd.Series(False, index=df.index)

def has_cols(df: pd.DataFrame, cols) -> bool:
    """å¿…è¦åˆ—ãŒã™ã¹ã¦å­˜åœ¨ã™ã‚‹ã‹"""
    return df is not None and set(cols).issubset(set(df.columns))

# --- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ ---
st.sidebar.header("1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
st.sidebar.info("åˆ†æå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’2ã¤ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
uploaded_seisakubutsu_file = st.sidebar.file_uploader("åˆ¶ä½œç‰©ä¸€è¦§ CSV", type="csv")
uploaded_header_file = st.sidebar.file_uploader("ãƒ˜ãƒƒãƒ€ãƒ¼ä¸€è¦§ CSV", type="csv")

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆéã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ ---
def load_data(seisakubutsu_file, header_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰CSVã‚’èª­ã¿è¾¼ã¿ã€çµåˆãƒ»å‰å‡¦ç†ã‚’è¡Œã†ï¼ˆPIIã¯æ—©æœŸé™¤å»ï¼‰"""
    try:
        seisakubutsu_df = pd.read_csv(seisakubutsu_file)
        header_df = pd.read_csv(header_file)
    except Exception:
        st.error("ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚CSVå½¢å¼ã‚„æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        return None, None

    # æ—¥ä»˜æ­£è¦åŒ–
    for col in ['ä½œæˆæ—¥', 'ä¿®æ­£æ—¥', 'ç· ã‚åˆ‡ã‚Šæ—¥']:
        if col in seisakubutsu_df.columns:
            seisakubutsu_df[col] = pd.to_datetime(seisakubutsu_df[col], errors='coerce').dt.tz_localize(None)
        if col in header_df.columns:
            header_df[col] = pd.to_datetime(header_df[col], errors='coerce').dt.tz_localize(None)

    # åˆ—åæ•´å½¢ & ãƒã‚§ãƒƒã‚¯è€…æ•°é›†è¨ˆ
    header_df.rename(columns={'åˆ¶ä½œç‰©ãƒˆãƒ¼ã‚¯ãƒ³': 'ãƒˆãƒ¼ã‚¯ãƒ³'}, inplace=True)
    if 'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹' in header_df.columns:
        checkers_count_df = header_df.groupby('ãƒˆãƒ¼ã‚¯ãƒ³')['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'].nunique().reset_index()
        checkers_count_df.rename(columns={'æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹': 'ãƒã‚§ãƒƒã‚¯è€…æ•°'}, inplace=True)
        # PIIã¯é€Ÿã‚„ã‹ã«å‰Šé™¤
        # header_df = header_df.drop(columns=['æ‹…å½“è€…ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'])
    else:
        checkers_count_df = pd.DataFrame(columns=['ãƒˆãƒ¼ã‚¯ãƒ³', 'ãƒã‚§ãƒƒã‚¯è€…æ•°'])

    # åˆ¶ä½œç‰©å´ã¸ãƒã‚§ãƒƒã‚¯è€…æ•°ã‚’ä»˜ä¸
    if 'ãƒˆãƒ¼ã‚¯ãƒ³' in seisakubutsu_df.columns:
        seisakubutsu_df = pd.merge(seisakubutsu_df, checkers_count_df, on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left')
        seisakubutsu_df['ãƒã‚§ãƒƒã‚¯è€…æ•°'] = seisakubutsu_df['ãƒã‚§ãƒƒã‚¯è€…æ•°'].fillna(0)
    else:
        seisakubutsu_df['ãƒã‚§ãƒƒã‚¯è€…æ•°'] = 0

    # çµåˆ
    if 'ãƒˆãƒ¼ã‚¯ãƒ³' in header_df.columns and 'ãƒˆãƒ¼ã‚¯ãƒ³' in seisakubutsu_df.columns:
        merged_df = pd.merge(header_df, seisakubutsu_df, on='ãƒˆãƒ¼ã‚¯ãƒ³',
                             suffixes=('_header', '_seisakubutsu'))
    else:
        st.error("ã‚¨ãƒ©ãƒ¼: åŒæ–¹ã®CSVã«ã€ãƒˆãƒ¼ã‚¯ãƒ³ã€åˆ—ãŒå¿…è¦ã§ã™ã€‚")
        return None, None

    # ç™ºåˆŠæœˆã®ã‚«ãƒ†ã‚´ãƒª
    for df in (merged_df, seisakubutsu_df):
        if 'ç™ºåˆŠæœˆ' in df.columns:
            df['ç™ºåˆŠæœˆ'] = pd.Categorical(df['ç™ºåˆŠæœˆ'], categories=month_order, ordered=True)

    # å®Ÿãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹å·¥ç¨‹ã®ã¿è¨±å®¹
    existing_processes = pd.concat([
        merged_df['å·¥ç¨‹'] if 'å·¥ç¨‹' in merged_df.columns else pd.Series(dtype=str),
        seisakubutsu_df['å·¥ç¨‹'] if 'å·¥ç¨‹' in seisakubutsu_df.columns else pd.Series(dtype=str)
    ]).dropna().unique()
    filtered_process_order = [p for p in original_process_order if p in existing_processes]
    for df in (merged_df, seisakubutsu_df):
        if 'å·¥ç¨‹' in df.columns:
            df['å·¥ç¨‹'] = pd.Categorical(df['å·¥ç¨‹'], categories=filtered_process_order, ordered=True)

    return merged_df, seisakubutsu_df

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if uploaded_seisakubutsu_file is not None and uploaded_header_file is not None:
    df_merged_all, df_seisakubutsu_all = load_data(uploaded_seisakubutsu_file, uploaded_header_file)
    if df_merged_all is None:
        st.stop()

    st.sidebar.header("2. çµã‚Šè¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    if 'ä½œæˆæ—¥' not in df_seisakubutsu_all.columns or df_seisakubutsu_all['ä½œæˆæ—¥'].dropna().empty:
        st.error("ã‚¨ãƒ©ãƒ¼: åˆ¶ä½œç‰©CSVã«ã€ä½œæˆæ—¥ã€åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã¾ãŸã¯å…¨ã¦æ¬ æï¼‰")
        st.stop()

    min_date = df_seisakubutsu_all['ä½œæˆæ—¥'].min().date()
    max_date = df_seisakubutsu_all['ä½œæˆæ—¥'].max().date()
    start_date = st.sidebar.date_input('é–‹å§‹æ—¥', min_date, min_value=min_date, max_value=max_date)
    end_date = st.sidebar.date_input('çµ‚äº†æ—¥', max_date, min_value=start_date, max_value=max_date)

    if start_date > end_date:
        st.sidebar.error('ã‚¨ãƒ©ãƒ¼: çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚')
        st.stop()

    start_datetime = pd.to_datetime(start_date)
    end_datetime = pd.to_datetime(end_date) + pd.Timedelta(days=1)

    # æœŸé–“é©ç”¨ï¼ˆçµåˆå´ã¯ä½œæˆæ—¥_seisakubutsuï¼‰
    date_col_merged = 'ä½œæˆæ—¥_seisakubutsu'
    if date_col_merged not in df_merged_all.columns:
        st.error("ã‚¨ãƒ©ãƒ¼: çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ã«ã€ä½œæˆæ—¥_seisakubutsuã€ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¥åŠ›CSVã®åˆ—åã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        st.stop()

    df_filtered_by_date = df_merged_all[
        (df_merged_all[date_col_merged] >= start_datetime) &
        (df_merged_all[date_col_merged] < end_datetime)
    ].copy()
    df_seisakubutsu_filtered_by_date = df_seisakubutsu_all[
        (df_seisakubutsu_all['ä½œæˆæ—¥'] >= start_datetime) &
        (df_seisakubutsu_all['ä½œæˆæ—¥'] < end_datetime)
    ].copy()

    # ç™ºåˆŠå¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.subheader("ç™ºåˆŠå¹´åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    available_years = sorted(df_seisakubutsu_filtered_by_date['å¹´åº¦'].dropna().unique().tolist()) \
        if 'å¹´åº¦' in df_seisakubutsu_filtered_by_date.columns else []
    selected_year = st.sidebar.selectbox('æ¯”è¼ƒã—ãŸã„ç™ºåˆŠå¹´åº¦ã‚’é¸æŠ', options=['ã™ã¹ã¦'] + available_years)

    # ç™ºåˆŠæœˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.subheader("ç™ºåˆŠæœˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    if 'ç™ºåˆŠæœˆ' in df_seisakubutsu_filtered_by_date.columns:
        available_months = [m for m in month_order if m in df_seisakubutsu_filtered_by_date['ç™ºåˆŠæœˆ'].dropna().unique()]
    else:
        available_months = []
    selected_month = st.sidebar.selectbox('æ¯”è¼ƒã—ãŸã„ç™ºåˆŠæœˆã‚’é¸æŠ', options=['ã™ã¹ã¦'] + available_months)

    df_filtered_by_month = df_filtered_by_date.copy()
    df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_date.copy()
    if selected_year != 'ã™ã¹ã¦' and 'å¹´åº¦' in df_filtered_by_month.columns:
        df_filtered_by_month = df_filtered_by_month[df_filtered_by_month['å¹´åº¦'] == selected_year]
        df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['å¹´åº¦'] == selected_year]
    if selected_month != 'ã™ã¹ã¦' and 'ç™ºåˆŠæœˆ' in df_filtered_by_month.columns:
        df_filtered_by_month = df_filtered_by_month[df_filtered_by_month['ç™ºåˆŠæœˆ'] == selected_month]
        df_seisakubutsu_filtered_by_month = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['ç™ºåˆŠæœˆ'] == selected_month]

    # å­¦å¹´ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.subheader("å­¦å¹´ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    grade_cols = [c for c in df_seisakubutsu_all.columns
              if ('å¹´ç”Ÿ' in c or 'å­¦å¹´ãã®ä»–' in c or c == 'å…¥å­¦æº–å‚™')]
    if grade_cols:
        melted_grades = df_seisakubutsu_filtered_by_month.melt(
            id_vars=['ãƒˆãƒ¼ã‚¯ãƒ³'], value_vars=grade_cols, var_name='å­¦å¹´', value_name='å¯¾è±¡'
        )
        relevant_grades = melted_grades[melted_grades['å¯¾è±¡'] == True]
        available_grades = relevant_grades['å­¦å¹´'].unique().tolist()
    else:
        melted_grades = pd.DataFrame(columns=['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´', 'å¯¾è±¡'])
        relevant_grades = pd.DataFrame(columns=['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´'])
        available_grades = []

    selected_grades = st.sidebar.multiselect('åˆ†æã—ãŸã„å­¦å¹´ã‚’é¸æŠ', options=available_grades, default=available_grades)

    if selected_grades and not relevant_grades.empty:
        selected_tokens = relevant_grades[relevant_grades['å­¦å¹´'].isin(selected_grades)]['ãƒˆãƒ¼ã‚¯ãƒ³'].unique()
        df_filtered = df_filtered_by_month[df_filtered_by_month['ãƒˆãƒ¼ã‚¯ãƒ³'].isin(selected_tokens)].copy()
        df_seisakubutsu_filtered = df_seisakubutsu_filtered_by_month[df_seisakubutsu_filtered_by_month['ãƒˆãƒ¼ã‚¯ãƒ³'].isin(selected_tokens)].copy()
    else:
        df_filtered = pd.DataFrame(columns=df_filtered_by_month.columns)
        df_seisakubutsu_filtered = pd.DataFrame(columns=df_seisakubutsu_filtered_by_month.columns)

    # åˆ¶ä½œç‰©åãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.subheader("åˆ¶ä½œç‰©åãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    name_filter_text = st.sidebar.text_input('åˆ¶ä½œç‰©åã«å«ã¾ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã§çµã‚Šè¾¼ã¿')
    if name_filter_text and 'åˆ¶ä½œç‰©å' in df_filtered.columns:
        df_filtered = df_filtered[df_filtered['åˆ¶ä½œç‰©å'].str.contains(name_filter_text, na=False)].copy()
        df_seisakubutsu_filtered = df_seisakubutsu_filtered[df_seisakubutsu_filtered['åˆ¶ä½œç‰©å'].str.contains(name_filter_text, na=False)].copy()

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    st.sidebar.header("3. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    if not df_filtered.empty:
        output = io.BytesIO()
        df_filtered.to_csv(output, index=False, encoding='utf-8-sig')  # Exceläº’æ›
        st.sidebar.text("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®çµ±åˆCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        st.sidebar.download_button(
            label="â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=output.getvalue(),
            file_name='filtered_data.csv',
            mime='text/csv'
        )
        del output

    # è¡¨ç¤ºã‚¬ãƒ¼ãƒ‰
    if df_filtered.empty:
        st.warning("é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ã‚µãƒãƒªãƒ¼
    unique_items = df_seisakubutsu_filtered['åˆ¶ä½œç‰©å'].nunique() if 'åˆ¶ä½œç‰©å' in df_seisakubutsu_filtered.columns else 0
    st.success(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†ã€‚ç¾åœ¨ {unique_items} ä»¶ã®åˆ¶ä½œç‰©ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­ã§ã™ã€‚")

    processes_for_tabs = [p for p in original_process_order
                          if 'å·¥ç¨‹' in df_filtered.columns and p in df_filtered['å·¥ç¨‹'].unique()]

    # --- å…¨ä½“ã‚µãƒãƒªãƒ¼ ---
    st.text("")
    st.header("ğŸ“Š å…¨ä½“ã‚µãƒãƒªãƒ¼")
    st.markdown("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§çµã‚Šè¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®æ¦‚è¦ï¼ˆå­¦å¹´åˆ¥ãƒ»åˆè¨ˆï¼‰ã¨ã€ç™ºåˆŠæœˆã”ã¨ã®æ¨ç§»ã‚’ç¢ºèªã§ãã¾ã™ã€‚")

    # å­¦å¹´åˆ¥ã‚µãƒãƒªãƒ¼
    if not relevant_grades.empty:
        df_filtered_with_grade_summary = pd.merge(
            df_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(),
            on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left'
        )
        df_seisakubutsu_with_grade_summary = pd.merge(
            df_seisakubutsu_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(),
            on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left'
        )

        summary_items = df_seisakubutsu_with_grade_summary.groupby('å­¦å¹´', observed=True)['åˆ¶ä½œç‰©å'].nunique().rename('ç·åˆ¶ä½œç‰©ä»¶æ•°') \
            if 'åˆ¶ä½œç‰©å' in df_seisakubutsu_with_grade_summary.columns else pd.Series(dtype='float64')
        summary_processes = df_seisakubutsu_with_grade_summary.groupby('å­¦å¹´', observed=True).size().rename('ç·å·¥ç¨‹æ•°')

        # æœŸé™å†…å®Œäº†ç‡ã®è¨ˆç®—ï¼ˆå¿…è¦åˆ—ãŒæƒã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼‰
        need_cols = {'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', 'ä¿®æ­£æ—¥_header', 'ç· ã‚åˆ‡ã‚Šæ—¥'}
        if has_cols(df_filtered_with_grade_summary, need_cols):
            completed = df_filtered_with_grade_summary[safe_bool_series(df_filtered_with_grade_summary, 'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿')] \
                .groupby('å­¦å¹´', observed=True).size().rename('completed')
            ontime = df_filtered_with_grade_summary[
                safe_bool_series(df_filtered_with_grade_summary, 'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿') &
                (df_filtered_with_grade_summary['ä¿®æ­£æ—¥_header'] <= df_filtered_with_grade_summary['ç· ã‚åˆ‡ã‚Šæ—¥'])
            ].groupby('å­¦å¹´', observed=True).size().rename('on_time')
            on_time_summary = pd.concat([completed, ontime], axis=1)
            num_fillna_inplace(on_time_summary, 0)
            on_time_summary['æœŸé™å†…å®Œäº†ç‡(%)'] = (
                (on_time_summary.get('on_time', 0) / on_time_summary.get('completed', 0).replace(0, pd.NA)) * 100
            ).fillna(0)
        else:
            on_time_summary = pd.DataFrame({'æœŸé™å†…å®Œäº†ç‡(%)': []})

        # å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°
        if has_cols(df_seisakubutsu_with_grade_summary, {'å­¦å¹´', 'ãƒˆãƒ¼ã‚¯ãƒ³', 'ãƒã‚§ãƒƒã‚¯è€…æ•°'}):
            avg_checkers = df_seisakubutsu_with_grade_summary.drop_duplicates(subset=['å­¦å¹´', 'ãƒˆãƒ¼ã‚¯ãƒ³']) \
                .groupby('å­¦å¹´', observed=True)['ãƒã‚§ãƒƒã‚¯è€…æ•°'].mean().rename('å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)')
        else:
            avg_checkers = pd.Series(dtype='float64', name='å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)')

        summary_by_grade_df = pd.concat(
            [summary_items, summary_processes, on_time_summary.get('æœŸé™å†…å®Œäº†ç‡(%)', pd.Series(dtype='float64')), avg_checkers],
            axis=1
        ).reset_index()

        num_fillna_inplace(summary_by_grade_df, 0)

        # å…¨ä½“åˆè¨ˆ
        total_items = df_seisakubutsu_filtered['åˆ¶ä½œç‰©å'].nunique() if 'åˆ¶ä½œç‰©å' in df_seisakubutsu_filtered.columns else 0
        total_processes = len(df_seisakubutsu_filtered)
        if has_cols(df_filtered, {'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', 'ä¿®æ­£æ—¥_header', 'ç· ã‚åˆ‡ã‚Šæ—¥'}):
            total_completed = int(safe_bool_series(df_filtered, 'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿').sum())
            total_on_time = int((safe_bool_series(df_filtered, 'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿') &
                                (df_filtered['ä¿®æ­£æ—¥_header'] <= df_filtered['ç· ã‚åˆ‡ã‚Šæ—¥'])).sum())
            total_on_time_rate = (total_on_time / total_completed * 100) if total_completed > 0 else 0
        else:
            total_on_time_rate = 0
        total_avg_checkers = df_seisakubutsu_filtered.drop_duplicates(subset=['ãƒˆãƒ¼ã‚¯ãƒ³'])['ãƒã‚§ãƒƒã‚¯è€…æ•°'].mean() \
            if 'ãƒã‚§ãƒƒã‚¯è€…æ•°' in df_seisakubutsu_filtered.columns else 0

        total_summary_df = pd.DataFrame([{
            'å­¦å¹´': 'åˆè¨ˆ',
            'ç·åˆ¶ä½œç‰©ä»¶æ•°': total_items,
            'ç·å·¥ç¨‹æ•°': total_processes,
            'æœŸé™å†…å®Œäº†ç‡(%)': total_on_time_rate,
            'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)': total_avg_checkers
        }])

        final_summary_df = pd.concat([summary_by_grade_df, total_summary_df], ignore_index=True)
        num_fillna_inplace(final_summary_df, 0)
        for col in ['ç·åˆ¶ä½œç‰©ä»¶æ•°', 'ç·å·¥ç¨‹æ•°']:
            if col in final_summary_df.columns:
                final_summary_df[col] = final_summary_df[col].astype(int)
        final_summary_df = final_summary_df.round(1)

        st.subheader("å­¦å¹´åˆ¥ã‚µãƒãƒªãƒ¼")
        show_cols = ['å­¦å¹´', 'ç·åˆ¶ä½œç‰©ä»¶æ•°', 'ç·å·¥ç¨‹æ•°', 'æœŸé™å†…å®Œäº†ç‡(%)', 'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)']
        st.dataframe(final_summary_df[[c for c in show_cols if c in final_summary_df.columns]],
                     use_container_width=True)
    else:
        st.info("å­¦å¹´ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€å­¦å¹´åˆ¥ã‚µãƒãƒªãƒ¼ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    st.markdown("---")

    # ç™ºåˆŠæœˆã®æ¨ç§»
    st.subheader("ç™ºåˆŠæœˆã”ã¨ã®æ¨ç§»")
    if not relevant_grades.empty and 'ç™ºåˆŠæœˆ' in df_seisakubutsu_filtered.columns:
        monthly_by_grade = df_seisakubutsu_with_grade_summary.groupby(['ç™ºåˆŠæœˆ', 'å­¦å¹´'], observed=True).agg(
            åˆ¶ä½œç‰©ä»¶æ•°=('åˆ¶ä½œç‰©å', 'nunique') if 'åˆ¶ä½œç‰©å' in df_seisakubutsu_with_grade_summary.columns else ('å­¦å¹´', 'size'),
            ç·å·¥ç¨‹æ•°=('ãƒˆãƒ¼ã‚¯ãƒ³', 'size')
        ).reset_index()

        monthly_total = df_seisakubutsu_filtered.groupby('ç™ºåˆŠæœˆ', observed=True).agg(
            åˆ¶ä½œç‰©ä»¶æ•°=('åˆ¶ä½œç‰©å', 'nunique') if 'åˆ¶ä½œç‰©å' in df_seisakubutsu_filtered.columns else ('ãƒˆãƒ¼ã‚¯ãƒ³', 'size'),
            ç·å·¥ç¨‹æ•°=('ãƒˆãƒ¼ã‚¯ãƒ³', 'size')
        ).reset_index()
        monthly_total['å­¦å¹´'] = 'åˆè¨ˆ'

        monthly_summary = pd.concat([monthly_by_grade, monthly_total], ignore_index=True)
        monthly_summary['ç™ºåˆŠæœˆ'] = pd.Categorical(monthly_summary['ç™ºåˆŠæœˆ'], categories=month_order, ordered=True)
        monthly_summary_for_graph = monthly_summary[monthly_summary['ç™ºåˆŠæœˆ'] != 'ãã®ä»–'].copy().sort_values('ç™ºåˆŠæœˆ')

        if not monthly_summary_for_graph.empty:
            col1, col2 = st.columns(2)
            with col1:
                fig_a = px.line(monthly_summary_for_graph, x='ç™ºåˆŠæœˆ', y='åˆ¶ä½œç‰©ä»¶æ•°', color='å­¦å¹´',
                                title='ç™ºåˆŠæœˆã”ã¨ã®åˆ¶ä½œç‰©ä»¶æ•°', markers=True,
                                color_discrete_sequence=px.colors.qualitative.T10)
                fig_a.update_layout(xaxis_title=None, legend_title_text='å­¦å¹´')
                fig_a.update_xaxes(categoryorder='array', categoryarray=[m for m in month_order if m != 'ãã®ä»–'])
                st.plotly_chart(fig_a, use_container_width=True)
            with col2:
                fig_b = px.line(monthly_summary_for_graph, x='ç™ºåˆŠæœˆ', y='ç·å·¥ç¨‹æ•°', color='å­¦å¹´',
                                title='ç™ºåˆŠæœˆã”ã¨ã®ç·å·¥ç¨‹æ•°', markers=True,
                                color_discrete_sequence=px.colors.qualitative.T10)
                fig_b.update_layout(xaxis_title=None, legend_title_text='å­¦å¹´')
                fig_b.update_xaxes(categoryorder='array', categoryarray=[m for m in month_order if m != 'ãã®ä»–'])
                st.plotly_chart(fig_b, use_container_width=True)
        else:
            st.info("ç™ºåˆŠæœˆã”ã¨ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.info("å­¦å¹´ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ç™ºåˆŠæœˆã”ã¨ã®æ¨ç§»ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

    # --- å·¥ç¨‹åˆ¥ çµ±åˆåˆ†æ ---
    st.text("")
    st.header("ğŸ“Š å·¥ç¨‹åˆ¥ çµ±åˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.markdown("å„å·¥ç¨‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨è©³ç´°åˆ†æã‚’ã€ä»¥ä¸‹ã®ã‚¿ãƒ–ã§åˆ‡ã‚Šæ›¿ãˆã¦ç¢ºèªã§ãã¾ã™ã€‚")

    df_performance = pd.DataFrame()
    if not df_filtered.empty and not relevant_grades.empty and 'å·¥ç¨‹' in df_filtered.columns:
        df_filtered_with_grade = pd.merge(
            df_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left'
        )
        df_seisakubutsu_with_grade = pd.merge(
            df_seisakubutsu_filtered, relevant_grades[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(), on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left'
        )

        active_processes = processes_for_tabs
        if selected_grades and active_processes:
            scaffold_df = pd.MultiIndex.from_product([selected_grades, active_processes],
                                                     names=['å­¦å¹´', 'å·¥ç¨‹']).to_frame(index=False)

            total_process_count_df = df_seisakubutsu_with_grade.groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size() \
                .rename('ç·å·¥ç¨‹æ•°').reset_index()

            # æœŸé™å†…å®Œäº†ç‡ï¼ˆåˆ—å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‰
            if has_cols(df_filtered_with_grade, {'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿', 'ä¿®æ­£æ—¥_header', 'ç· ã‚åˆ‡ã‚Šæ—¥'}):
                completed_g = df_filtered_with_grade[safe_bool_series(df_filtered_with_grade, 'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿')] \
                    .groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size().rename('completed_count')
                ontime_g = df_filtered_with_grade[
                    safe_bool_series(df_filtered_with_grade, 'ãƒã‚§ãƒƒã‚¯æ¸ˆã¿') &
                    (df_filtered_with_grade['ä¿®æ­£æ—¥_header'] <= df_filtered_with_grade['ç· ã‚åˆ‡ã‚Šæ—¥'])
                ].groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True).size().rename('on_time_count')

                on_time_rate_df = pd.concat([completed_g, ontime_g], axis=1).reset_index()
                # æ•°å€¤åˆ—ã®ã¿åŸ‹ã‚ã‚‹
                for col in ['completed_count', 'on_time_count']:
                    if col in on_time_rate_df.columns:
                        on_time_rate_df[col] = on_time_rate_df[col].fillna(0).astype(int)

                if {'completed_count', 'on_time_count'}.issubset(on_time_rate_df.columns):
                    denom = on_time_rate_df['completed_count'].replace(0, pd.NA)
                    on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'] = (on_time_rate_df['on_time_count'] / denom * 100).fillna(0)
                else:
                    on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'] = 0

                on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'] = on_time_rate_df['æœŸé™å†…å®Œäº†ç‡(%)'] \
                    .replace([float('inf'), -float('inf')], 0)
            else:
                on_time_rate_df = pd.DataFrame(columns=['å­¦å¹´', 'å·¥ç¨‹', 'æœŸé™å†…å®Œäº†ç‡(%)'])

            # å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°
            if has_cols(df_filtered_with_grade, {'å­¦å¹´', 'å·¥ç¨‹', 'ãƒˆãƒ¼ã‚¯ãƒ³', 'ãƒã‚§ãƒƒã‚¯è€…æ•°'}):
                avg_checkers_df = df_filtered_with_grade[['å­¦å¹´', 'å·¥ç¨‹', 'ãƒˆãƒ¼ã‚¯ãƒ³', 'ãƒã‚§ãƒƒã‚¯è€…æ•°']] \
                    .drop_duplicates().groupby(['å­¦å¹´', 'å·¥ç¨‹'], observed=True)['ãƒã‚§ãƒƒã‚¯è€…æ•°'] \
                    .mean().rename('å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)').reset_index()
            else:
                avg_checkers_df = pd.DataFrame(columns=['å­¦å¹´', 'å·¥ç¨‹', 'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)'])

            # çµåˆã—ã¦æ•°å€¤åˆ—ã®ã¿NAåŸ‹ã‚
            df_performance = scaffold_df.merge(total_process_count_df, on=['å­¦å¹´', 'å·¥ç¨‹'], how='left') \
                                        .merge(on_time_rate_df[['å­¦å¹´', 'å·¥ç¨‹', 'æœŸé™å†…å®Œäº†ç‡(%)']], on=['å­¦å¹´', 'å·¥ç¨‹'], how='left') \
                                        .merge(avg_checkers_df, on=['å­¦å¹´', 'å·¥ç¨‹'], how='left')
            num_fillna_inplace(df_performance, 0)
            if 'ç·å·¥ç¨‹æ•°' in df_performance.columns:
                df_performance['ç·å·¥ç¨‹æ•°'] = df_performance['ç·å·¥ç¨‹æ•°'].astype(int)
            df_performance = df_performance.round(1)
            df_performance['å­¦å¹´'] = pd.Categorical(df_performance['å­¦å¹´'], categories=selected_grades, ordered=True)
            df_performance['å·¥ç¨‹'] = pd.Categorical(df_performance['å·¥ç¨‹'], categories=active_processes, ordered=True)
            df_performance = df_performance.sort_values(by=['å­¦å¹´', 'å·¥ç¨‹'])

    if processes_for_tabs:
        process_tabs = st.tabs(processes_for_tabs)
        for i, process_name in enumerate(processes_for_tabs):
            with process_tabs[i]:
                df_proc = df_filtered[df_filtered.get('å·¥ç¨‹') == process_name].copy() \
                    if 'å·¥ç¨‹' in df_filtered.columns else pd.DataFrame()
                df_proc_sei = df_seisakubutsu_filtered[df_seisakubutsu_filtered.get('å·¥ç¨‹') == process_name].copy() \
                    if 'å·¥ç¨‹' in df_seisakubutsu_filtered.columns else pd.DataFrame()

                if df_proc.empty:
                    st.info("ã“ã®å·¥ç¨‹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                    continue

                st.subheader("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚­ãƒ³ã‚°")
                if not df_performance.empty:
                    view = df_performance[df_performance['å·¥ç¨‹'] == process_name]
                    if not view.empty:
                        st.dataframe(view[['å­¦å¹´', 'ç·å·¥ç¨‹æ•°', 'æœŸé™å†…å®Œäº†ç‡(%)', 'å¹³å‡ãƒã‚§ãƒƒã‚¯è€…æ•°(äºº)']],
                                     use_container_width=True)
                    else:
                        st.info(f"{process_name} ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    st.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

                st.markdown("---")
                st.subheader("è©³ç´°åˆ†æ")

                # æ‰‹æˆ»ã‚Šåˆ¤å®šãƒ•ãƒ©ã‚°ï¼ˆå‚è€ƒï¼‰
                rework_defs = [p for p in original_process_order if
                               ('å†æ ¡' in p or 'å¿µæ ¡' in p or 'è‰²æ ¡' in p or 'Î±' in p or 'Î²' in p)]
                if 'å·¥ç¨‹' in df_proc_sei.columns:
                    df_proc_sei['æ‰‹æˆ»ã‚Š'] = df_proc_sei['å·¥ç¨‹'].isin(rework_defs)

                st.markdown("**å­¦å¹´åˆ¥ã®ã€æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—ã€çŠ¶æ³**")
                if grade_cols and not df_proc_sei.empty:
                    melted_cur = df_proc_sei.melt(id_vars=['ãƒˆãƒ¼ã‚¯ãƒ³'], value_vars=grade_cols,
                                                  var_name='å­¦å¹´', value_name='å¯¾è±¡')
                    rel_cur = melted_cur[melted_cur['å¯¾è±¡'] == True]
                    df_next = pd.merge(df_proc, rel_cur[['ãƒˆãƒ¼ã‚¯ãƒ³', 'å­¦å¹´']].drop_duplicates(),
                                       on='ãƒˆãƒ¼ã‚¯ãƒ³', how='left') if not rel_cur.empty else pd.DataFrame(columns=['å­¦å¹´','æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—'])

                    scaffold_grades_df = pd.DataFrame({'å­¦å¹´': selected_grades})
                    if not df_next.empty and 'å­¦å¹´' in df_next.columns and 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—' in df_next.columns:
                        grouped = df_next.groupby(['å­¦å¹´'], observed=True)['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—']
                        count = grouped.sum().astype(int).rename('æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°')
                        total_in_group = grouped.size()
                        ratio = (count / total_in_group * 100).round(1).rename('æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)')
                        result_df = pd.concat([count, ratio], axis=1).reset_index()
                    else:
                        result_df = pd.DataFrame(columns=['å­¦å¹´', 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°', 'æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)'])

                    # æ•°å€¤åˆ—ã®ã¿NAåŸ‹ã‚
                    num_fillna_inplace(result_df, 0)
                    if not result_df.empty:
                        result_df['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°'] = result_df['æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°'].astype(int)
                        result_df['å­¦å¹´'] = pd.Categorical(result_df['å­¦å¹´'], categories=selected_grades, ordered=True)
                        result_df = result_df.sort_values(by='å­¦å¹´')

                        fig_ratio = px.bar(result_df, x='å­¦å¹´', y='æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_å‰²åˆ(%)',
                                           title='å­¦å¹´åˆ¥ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦ã€ã®å‰²åˆ', text_auto='.1f', color='å­¦å¹´')
                        fig_ratio.update_layout(showlegend=False, xaxis_title=None)
                        st.plotly_chart(fig_ratio, use_container_width=True, key=f"ratio_chart_{process_name}")

                        fig_count = px.bar(result_df, x='å­¦å¹´', y='æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦_äººæ•°',
                                           title='å­¦å¹´åˆ¥ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—è¦ã€ã®äººæ•°', text_auto=True, color='å­¦å¹´')
                        fig_count.update_layout(showlegend=False, xaxis_title=None)
                        st.plotly_chart(fig_count, use_container_width=True, key=f"count_chart_{process_name}")
                    else:
                        st.info("ã€Œæ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—ã€ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    st.info("å­¦å¹´åˆ—ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ¬¡å›ãƒã‚§ãƒƒã‚¯å‡ºã—çŠ¶æ³ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
    else:
        st.info("é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«è©²å½“ã™ã‚‹å·¥ç¨‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    # å¾Œå‡¦ç†ï¼ˆå¤§ããªDFã‚’è§£æ”¾ï¼‰
    del df_merged_all, df_seisakubutsu_all
    del df_filtered_by_date, df_seisakubutsu_filtered_by_date
    del df_filtered_by_month, df_seisakubutsu_filtered_by_month
    del df_filtered, df_seisakubutsu_filtered
    gc.collect()

else:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ†æå¯¾è±¡ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’2ã¤ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€åˆ†æãŒå§‹ã¾ã‚Šã¾ã™ã€‚")
